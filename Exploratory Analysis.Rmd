---
title: "Exploratory Analysis"
author: "Ck"
date: "April 3, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is the week 2 milestone report for the capstone project of the Coursera Data Scienence Specialization. The overal goal of the capstone is to develop a prediction algorithem for the most likely next word in a sequence of words. The purpose of this report is to demonstrate how data was downloaded, imported into R and cleaned. Furthermore it contains some exploratory analyses to investigate some features of the data.


## Data and Summary
Load data for US: blogs, news and twitter

```{r data}
suppressWarnings(library(stringi))
suppressWarnings(library(tm))
suppressWarnings(library(NLP))
suppressWarnings(library(RWeka))
suppressWarnings(library(ggplot2))

blogstext<-readLines("C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/final/en_US/en_US.blogs.txt",warn=FALSE,encoding="UTF-8")
newstext<-readLines("C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/final/en_US/en_US.news.txt",warn=FALSE,encoding="UTF-8")
twittertext<-readLines("C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/final/en_US/en_US.twitter.txt",warn=FALSE,encoding="UTF-8")
```

Summary table for the data showing file size, file length, number of characters and number of words

```{r summary}
size_blogs<-file.size(path="C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/final/en_US/en_US.blogs.txt")/2^20
size_news<-file.size(path="C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/final/en_US/en_US.news.txt")/2^20
size_twitter<-file.size(path="C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/final/en_US/en_US.twitter.txt")/2^20

len_blogs<-length(blogstext)
len_news<-length(newstext)
len_twitter<-length(twittertext)

nchar_blogs<-sum(nchar(blogstext))
nchar_news<-sum(nchar(newstext))
nchar_twitter<-sum(nchar(twittertext))

nword_blogs<-stri_stats_latex(blogstext)[4]
nword_news<-stri_stats_latex(newstext)[4]
nword_twitter<-stri_stats_latex(twittertext)[4]

table<-data.frame("File Name"=c("Blogs","News","Twitter"),
                  "File Size(MB)"=c(size_blogs,size_news,size_twitter),
                  "Num of rows"=c(len_blogs,len_news,len_twitter),
                  "Num of character"=c(nchar_blogs,nchar_news,nchar_twitter),
                  "Num of words"=c(nword_blogs,nword_news,nword_twitter))
table
```

## Data Cleaning

```{r clean data}
set.seed(12345)
blog_new<-iconv(blogstext,"latin1","ASCII",sub="")
news_new<-iconv(newstext,"latin1","ASCII",sub="")
twitter_new<-iconv(twittertext,"latin1","ASCII",sub="")


# sample data set only 1% of each file
sample_data<-c(sample(blog_new,length(blog_new)*0.01),
               sample(news_new,length(news_new)*0.01),
               sample(twitter_new,length(twitter_new)*0.01))
```

```{r echo=FALSE}
rm(blog_new)
rm(blogstext)
rm(news_new)
rm(newstext)
rm(twitter_new)
rm(twittertext)

```

## Selection of Corpus
```{r corpus}
corpus<-VCorpus(VectorSource(sample_data))
corpus1<-tm_map(corpus,removePunctuation)
corpus2<-tm_map(corpus1,stripWhitespace)
corpus3<-tm_map(corpus2,tolower)
corpus4<-tm_map(corpus3,removeNumbers)
corpus5<-tm_map(corpus4,PlainTextDocument)
corpus6<-tm_map(corpus5,removeWords,stopwords("english"))
corpus_result<-data.frame(text=unlist(sapply(corpus6,'[',"content")),stringsAsFactors = FALSE)

head(corpus_result)
```

```{r echo=FALSE}
rm(corpus)
rm(corpus1)
rm(corpus2)
rm(corpus3)
rm(corpus4)
rm(corpus5)
```

## Frequency Calculation of N-grams

Extract the words and frequencies of N-grams. Also plot their graphs.
```{r N-gram}
one<-function(x) NGramTokenizer(x,Weka_control(min=1,max=1))
two<-function(x) NGramTokenizer(x,Weka_control(min=2,max=2))
thr<-function(x) NGramTokenizer(x,Weka_control(min=3,max=3))
four<-function(x) NGramTokenizer(x,Weka_control(min=4,max=4))

one_table<-TermDocumentMatrix(corpus6,control=list(tokenize=one))
two_table<-TermDocumentMatrix(corpus6,control=list(tokenize=two))
thr_table<-TermDocumentMatrix(corpus6,control=list(tokenize=thr))
four_table<-TermDocumentMatrix(corpus6,control=list(tokenize=four))

one_corpus<-findFreqTerms(one_table,lowfreq=1000)
two_corpus<-findFreqTerms(two_table,lowfreq=80)
thr_corpus<-findFreqTerms(thr_table,lowfreq=10)
four_corpus<-findFreqTerms(four_table,lowfreq=5)

one_corpus_num<-rowSums(as.matrix(one_table[one_corpus,]))
one_corpus_table<-data.frame(Word=names(one_corpus_num),frequency=one_corpus_num)
one_corpus_sort<-one_corpus_table[order(-one_corpus_table$frequency),]

head(one_corpus_sort)
unigram <- data.frame(rows=names(one_corpus_num),count=one_corpus_num)
unigram$rows <- as.character(unigram$rows)
unigram_split <- strsplit(as.character(unigram$rows),split=" ")
unigram <- transform(unigram,first = sapply(unigram_split,"[[",1))
unigram <- data.frame(unigram = unigram$first,freq = unigram$count,stringsAsFactors=FALSE)
write.csv(unigram[unigram$freq > 1,],"C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/unigram",row.names=F)
unigram <- read.csv("C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/unigram",stringsAsFactors = F)
saveRDS(unigram,"C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/PredictTextCapstone/unigram.RData")

one_g<-ggplot(one_corpus_sort[1:10,],aes(x=reorder(Word,-frequency),y=frequency,fill=frequency))
one_g<-one_g+geom_bar(stat="identity")
one_g<-one_g+labs(title="Unigrams",x="Words",y="Frequency")
one_g<-one_g+theme(axis.text.x=element_text(angle=90))

one_g

two_corpus_num<-rowSums(as.matrix(two_table[two_corpus,]))
two_corpus_table<-data.frame(Word=names(two_corpus_num),frequency=two_corpus_num)
two_corpus_sort<-two_corpus_table[order(-two_corpus_table$frequency),]

head(two_corpus_sort)
bigram <- data.frame(rows=names(two_corpus_num),count=two_corpus_num)
bigram$rows <- as.character(bigram$rows)
bigram_split <- strsplit(as.character(bigram$rows),split=" ")
bigram <- transform(bigram,first = sapply(bigram_split,"[[",1),second = sapply(bigram_split,"[[",2))
bigram <- data.frame(unigram = bigram$first,bigram = bigram$second,freq = bigram$count,stringsAsFactors=FALSE)
write.csv(bigram[bigram$freq > 1,],"C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/bigram.csv",row.names=F)
bigram <- read.csv("C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/bigram.csv",stringsAsFactors = F)
saveRDS(bigram,"C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/PredictTextCapstone/bigram.RData")

two_g<-ggplot(two_corpus_sort[1:10,],aes(x=reorder(Word,-frequency),y=frequency,fill=frequency))
two_g<-two_g+geom_bar(stat="identity")
two_g<-two_g+labs(title="Bigrams",x="Words",y="Frequency")
two_g<-two_g+theme(axis.text.x=element_text(angle=90))

two_g

thr_corpus_num<-rowSums(as.matrix(thr_table[thr_corpus,]))
thr_corpus_table<-data.frame(Word=names(thr_corpus_num),frequency=thr_corpus_num)
thr_corpus_sort<-thr_corpus_table[order(-thr_corpus_table$frequency),]

head(thr_corpus_sort)
trigram <- data.frame(rows=names(thr_corpus_num),count=thr_corpus_num)
trigram$rows <- as.character(trigram$rows)
trigram_split <- strsplit(as.character(trigram$rows),split=" ")
trigram <- transform(trigram,first = sapply(trigram_split,"[[",1),second = sapply(trigram_split,"[[",2),third = sapply(trigram_split,"[[",3))
trigram <- data.frame(unigram = trigram$first,bigram = trigram$second, trigram = trigram$third, freq = trigram$count,stringsAsFactors=FALSE)
write.csv(trigram[trigram$freq > 1,],"C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/trigram.csv",row.names=F)
trigram <- read.csv("C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/trigram.csv",stringsAsFactors = F)
saveRDS(trigram,"C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/PredictTextCapstone/trigram.RData")

thr_g<-ggplot(thr_corpus_sort[1:10,],aes(x=reorder(Word,-frequency),y=frequency,fill=frequency))
thr_g<-thr_g+geom_bar(stat="identity")
thr_g<-thr_g+labs(title="Trigrams",x="Words",y="Frequency")
thr_g<-thr_g+theme(axis.text.x=element_text(angle=90))

thr_g


four_corpus_num<-rowSums(as.matrix(four_table[four_corpus,]))
four_corpus_table<-data.frame(Word=names(four_corpus_num),frequency=four_corpus_num)
four_corpus_sort<-four_corpus_table[order(-four_corpus_table$frequency),]

quadgram <- data.frame(rows=names(four_corpus_num),count=four_corpus_num)
quadgram$rows <- as.character(quadgram$rows)
quadgram_split <- strsplit(as.character(quadgram$rows),split=" ")
quadgram <- transform(quadgram,first = sapply(quadgram_split,"[[",1),second = sapply(quadgram_split,"[[",2),third = sapply(quadgram_split,"[[",3), fourth = sapply(quadgram_split,"[[",4))
quadgram <- data.frame(unigram = quadgram$first,bigram = quadgram$second, trigram = quadgram$third, quadgram = quadgram$fourth, freq = quadgram$count,stringsAsFactors=FALSE)
write.csv(quadgram[quadgram$freq > 1,],"C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/quadgram.csv",row.names=F)
quadgram <- read.csv("C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/quadgram.csv",stringsAsFactors = F)
saveRDS(quadgram,"C:/Users/BTCZ5713/Desktop/Data Science/Capstone Project/PredictTextCapstone/quadgram.RData")

```

## Conclusion
This concludes our exploratory analysis. The next steps of this capstone project would be to finalize our predictive algorithm, and deploy our algorithm as a Shiny app.

the predictive algorithm comprise of the n-gram model with frequency lookup similar to the exploratory analysis above. Trigram model is the first priority to lookup for the predicted words, and follow by bigrams and unigrams. This means that if no matching trigram can be found, then the algorithm would back off to the bigram model, and then to the unigram model if needed.

The user interface of the Shiny app will consist of a text input box that will allow a user to enter a phrase. Then the app will use our algorithm to suggest the most likely next word after a short delay. Our plan is also to allow the user to configure how many words our app should suggest.